{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准库\n",
    "import argparse\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "# 第三方库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 自定义库\n",
    "from evaluate import evaluate_model\n",
    "import utils\n",
    "from dataset import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 是否激活cuda\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.is_available())\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DMF(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, num_users, num_items, layers, dataset):\n",
    "        super(DMF, self).__init__()\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.latent_dim = layers[0]\n",
    "        self.layers = layers\n",
    "\n",
    "        self.user_item_indices = torch.LongTensor(np.array([dataset.user_indices, dataset.item_incides]))\n",
    "        self.rating_data = torch.FloatTensor(dataset.rating_data)\n",
    "        self.user_item_matrix = torch.sparse_coo_tensor(self.user_item_indices, self.rating_data,\n",
    "                                                        torch.Size((self.num_users, self.num_items))).to_dense().to(device)\n",
    "\n",
    "        self.linear_user_1 = nn.Linear(in_features=self.num_items, out_features=self.latent_dim)\n",
    "        self.linear_user_1.weight.detach().normal_(0, 0.01)\n",
    "        self.linear_item_1 = nn.Linear(in_features=self.num_users, out_features=self.latent_dim)\n",
    "        self.linear_item_1.weight.detach().normal_(0, 0.01)\n",
    "        # 想办法添加自编码器\n",
    "\n",
    "        self.user_fc_layers = nn.ModuleList()\n",
    "        for idx in range(1, len(self.layers)):\n",
    "            self.user_fc_layers.append(nn.Linear(in_features=self.layers[idx - 1], out_features=self.layers[idx]))\n",
    "\n",
    "        self.item_fc_layers = nn.ModuleList()\n",
    "        for idx in range(1, len(self.layers)):\n",
    "            self.item_fc_layers.append(nn.Linear(in_features=self.layers[idx - 1], out_features=self.layers[idx]))\n",
    "\n",
    "    def forward(self, user_indices, item_indices):#user_indices,item_indices代表数据集中的user 和 item 并把rating置为1\n",
    "\n",
    "        user = self.user_item_matrix[user_indices]\n",
    "        item = self.user_item_matrix[:, item_indices].t()\n",
    "\n",
    "        user = self.linear_user_1(user)\n",
    "        item = self.linear_item_1(item)\n",
    "\n",
    "        for idx in range(len(self.layers) - 1):\n",
    "            user = F.relu(user)\n",
    "            user = self.user_fc_layers[idx](user)\n",
    "\n",
    "        for idx in range(len(self.layers) - 1):\n",
    "            item = F.relu(item)\n",
    "            item = self.item_fc_layers[idx](item)\n",
    "\n",
    "        vector = torch.cosine_similarity(user, item).view(-1, 1)\n",
    "        vector = torch.clamp(vector, min=0, max=2)\n",
    "        # print(\"模型输出vector的shape：\")\n",
    "        # print(vector.shape)\n",
    "\n",
    "        return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################### Arguments ####################\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Run Conv1.\")\n",
    "parser.add_argument('--path', nargs='?', default='data/',\n",
    "                        help='Input data path.')\n",
    "# parser.add_argument('--path', nargs='?', default='./data/',\n",
    "#                         help='Input data path.')\n",
    "# parser.add_argument('--dataset', nargs='?', default='OLIES2011',\n",
    "#                         help='Choose a dataset.')\n",
    "# parser.add_argument('--dataset', nargs='?', default='L_DMF_input',\n",
    "#                         help='Choose a dataset.')\n",
    "parser.add_argument('--dataset', nargs='?', default='assistment2009',\n",
    "                        help='Choose a dataset.')\n",
    "parser.add_argument('--epochs', type=int, default=20,\n",
    "                        help='Number of epochs.')\n",
    "parser.add_argument('--batch_size', type=int, default=256,\n",
    "                        help='Batch size.')\n",
    "parser.add_argument('--num_factors', type=int, default=64,\n",
    "                        help='Embedding size.')\n",
    "parser.add_argument('--layers', nargs='?', default='[64,64]',\n",
    "                        help=\"Size of each layer. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
    "parser.add_argument('--reg', type=float, default='0.0',\n",
    "                        help=\"Regularization for each layer\")\n",
    "parser.add_argument('--num_neg', type=int, default=1,\n",
    "                        help='Number of negative instances to pair with a positive instance.')\n",
    "parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                        help='Learning rate.')\n",
    "parser.add_argument('--learner', nargs='?', default='adam',\n",
    "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
    "parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Show performance per X iterations')\n",
    "parser.add_argument('--out', type=int, default=1,\n",
    "                        help='Whether to save the trained model.')\n",
    "parser.add_argument('--emlp_pretrain', nargs='?', default='',\n",
    "                        help='Specify the pretrain model file for MLP part. If empty, no pretrain will be used')\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "args =parser.parse_known_args()[0]\n",
    "args = parser.parse_args(args=[])\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # settings\n",
    "    \n",
    "    path = args.path\n",
    "    dataset_name = args.dataset\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    layers = eval(args.layers)\n",
    "    latent_dim = layers[0]\n",
    "    # \n",
    "    # print(\"latent_dim\")\n",
    "    # print(latent_dim)\n",
    "    reg = args.reg\n",
    "    learning_rate = args.lr\n",
    "    num_negative = args.num_neg\n",
    "    verbose = args.verbose\n",
    "    out = args.out\n",
    "    emlp_pretrain = args.emlp_pretrain\n",
    "    topK = 10\n",
    "    evaluation_threads = 1  # mp.cpu_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试集的生成\n",
    "# data3 = pd.read_csv(r\"C:\\Users\\wangxin\\Desktop\\DMF-vscode\\data\\OLIES2011_rating.csv\",sep=\" \").to_numpy()\n",
    "# # print(data3)\n",
    "# stu=0\n",
    "# a=[]#做过的题目\n",
    "# b=[]\n",
    "# test0=[]\n",
    "# train0=[]\n",
    "# for i in data3:\n",
    "\n",
    "    \n",
    "#     if stu==i[0]:\n",
    "#         # test0.append(i)\n",
    "#         a.append(i)             #暂存\n",
    "#         b=i\n",
    "#         # stu=i[0]\n",
    "#     else:\n",
    "#         # train0.append(i)\n",
    "#         if len(a)>=2:               #学习记录至少两条的情况\n",
    "#             c=random.randint(0,len(a)-1)        #随机取出一条\n",
    "#             test0.append(a[c])              #放到测试集\n",
    "#             for x in range(len(a)):\n",
    "#                 if x!=c:\n",
    "#                     train0.append(a[x])    #遍历剩余的，放到训练集\n",
    "#                 # else:\n",
    "#                 #     continue\n",
    "            \n",
    "#             a=[]\n",
    "#             a.append(i) \n",
    "\n",
    "#         else:\n",
    "#             # print(type(i))\n",
    "#             # # print(type(a))\n",
    "#             # print(i)\n",
    "#             # print(a)\n",
    "#             # print(np.array(a))\n",
    "#             test0.append(a[0])\n",
    "#             train0.append(a[0])\n",
    "#             a=[]\n",
    "#             a.append(i) \n",
    "    \n",
    "#         stu=i[0]\n",
    "    \n",
    "# # print(pd.DataFrame(test0))\n",
    "# # print(train0)\n",
    "# train0=pd.DataFrame(train0)\n",
    "# train0.to_csv('OLIES2011.train.rating',index=False)\n",
    "# test0=pd.DataFrame(test0)\n",
    "# test0.to_csv('OLIES2011.test.rating',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# data3 =pd.read_csv(\"OLIES2011_rating.csv\",sep=',').to_numpy()\n",
    "# print(type(data3))\n",
    "# for i in tqdm.tqdm(data3):\n",
    "\n",
    "#     print(\"i[0]:\",i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 李磊测试集的生成\n",
    "# import tqdm\n",
    "# data3 = pd.read_csv(\"./data/L_DMF_input_correct.rating\",sep=',').to_numpy()\n",
    "# # print(data3)\n",
    "# stu=1\n",
    "# a=[]#做过的题目\n",
    "# b=[]\n",
    "# test0=[]\n",
    "# train0=[]\n",
    "# for i in tqdm.tqdm(data3):\n",
    "\n",
    "#     # print(\"i[0]:\",i[0])\n",
    "#     if stu==int(i[0]):\n",
    "#         # test0.append(i)\n",
    "#         a.append(i)             #暂存\n",
    "#         # print(\"a:\",a)\n",
    "#         b=i\n",
    "#         # stu=i[0]\n",
    "#     else:\n",
    "#         # train0.append(i)\n",
    "#         # if len(a)>=2:               #学习记录至少两条的情况\n",
    "#                 #随机取出一条\n",
    "        \n",
    "#         # print(\"len(a):\",len(a))\n",
    "#         # print(\"c\",c)\n",
    "#         c=random.randint(0,len(a)-1)\n",
    "#         test0.append(a[c])              #放到测试集\n",
    "#         for x in range(len(a)):\n",
    "#             if x!=c:\n",
    "#                 train0.append(a[x])    #遍历剩余的，放到训练集\n",
    "#                 # else:\n",
    "#                 #     continue\n",
    "            \n",
    "#         a=[]\n",
    "#         a.append(i) \n",
    "\n",
    "#         # else:\n",
    "#         #     # print(type(i))\n",
    "#         #     # # print(type(a))\n",
    "#         #     # print(i)\n",
    "#         #     # print(a)\n",
    "#         #     # print(np.array(a))\n",
    "#         #     test0.append(a[0])\n",
    "#         #     train0.append(a[0])\n",
    "#         #     a=[]\n",
    "#         #     a.append(i) \n",
    "    \n",
    "#         stu=i[0]\n",
    "#         # print(\"stu:\",stu)\n",
    "# x=[]\n",
    "# y=[]\n",
    "# for i in data3:\n",
    "#     if i[0]==6866:\n",
    "#         x.append(i)\n",
    "# c=random.randint(0,len(x)-1)        #随机取出一条\n",
    "#         # print(\"len(a):\",len(a))\n",
    "#         # print(\"c\",c)\n",
    "# test0.append(x[c])              #放到测试集\n",
    "# for j in range(len(x)):\n",
    "#     if j!=c:\n",
    "#         train0.append(x[j])\n",
    "# # print(pd.DataFrame(test0))\n",
    "# # print(train0)\n",
    "# train0=pd.DataFrame(train0)\n",
    "# train0.to_csv('./data/L_DMF_input_correct.train.rating',index=False)\n",
    "# # train0.to_csv('./data/L_DMF_input.train.csv',index=False)\n",
    "# test0=pd.DataFrame(test0)\n",
    "# test0.to_csv('./data/L_DMF_input_correct.test.rating',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 数据集的处理\n",
    "# data1 = pd.read_csv(r\"C:\\Users\\wangxin\\Desktop\\DMF-vscode\\data\\OLIES2011_rating.csv\",sep=' ').to_numpy()\n",
    "\n",
    "\n",
    "# x_train,x_test= train_test_split(data1,test_size=0.05,random_state=0)\n",
    "\n",
    "# print(x_train)\n",
    "\n",
    "# oli_train=pd.DataFrame(x_train)\n",
    "# oli_train.to_csv('OLIES2011.train.rating',index=False)\n",
    "# oli_test=pd.DataFrame(x_test)\n",
    "# oli_test.to_csv('OLIES2011.test.rating',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # negative的生成\n",
    "# data2 = pd.read_csv(r\"C:\\Users\\wangxin\\Desktop\\DMF-vscode\\data\\OLIES2011_rating.csv\",sep=\" \").to_numpy()\n",
    "# # print(data2)\n",
    "# negative=[]     #没做过的题题目\n",
    "# old_t=[]        #做过的题目\n",
    "# stu=0          #学生编号\n",
    "# all_t=[]        #所有题目\n",
    "# a=0\n",
    "# for i in data2:\n",
    "    \n",
    "#     all_t.append(i[1])\n",
    "# print(len(set(all_t)))\n",
    "# for i in data2:\n",
    "#     if stu==i[0]:\n",
    "#         old_t.append(i[1])\n",
    "#     else:\n",
    "#         stu=i[0]\n",
    "#         negative.append(set(all_t)-set(old_t))\n",
    "#         old_t=[]\n",
    "        \n",
    "#         # negative.append(i[1])\n",
    "#         continue\n",
    "\n",
    "# negative.append(set(all_t)-set(old_t))    \n",
    "# print(len(negative))\n",
    "# negative=pd.DataFrame(negative)\n",
    "# negative.to_csv(\"OLIES2011.test.negative\",index=False)\n",
    "# # print(negative.shape)\n",
    "# # print(negative)\n",
    "# # negative=set(negative)\n",
    "# # print(negative)\n",
    "# # print(old_t)\n",
    "# # print(len(negative))\n",
    "# # print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#李磊数据，negative的生成，全0\n",
    "# a=[]\n",
    "# for i in range(1,6866):\n",
    "#     a.append(\" \")\n",
    "# a=pd.DataFrame(a)\n",
    "# a.to_csv(\"./data/L_DMF_input.test.negative\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 李磊数据，negative的生成，做错的题\n",
    "# a=[]\n",
    "# for i in range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative的生成\n",
    "# 李磊数据，negative的生成，做错的题\n",
    "# data2 = pd.read_csv(r\"C:\\Users\\wangxin\\Desktop\\remmend_DKT_Apriori_DMF\\data\\L_DMF_input.csv\",sep=',').to_numpy()\n",
    "# # print(data2)\n",
    "# negative=[]     #做错的题题目\n",
    "# a=[]\n",
    "# b=1\n",
    "# import tqdm\n",
    "\n",
    "# for i in tqdm.tqdm(data2):\n",
    "#     if b==i[0]:\n",
    "#         if i[2]==0:\n",
    "#             a.append(i[1])\n",
    "#     else:\n",
    "#         b=i[0]\n",
    "#         negative.append(a)\n",
    "#         a=[]\n",
    "#         if i[2]==0:\n",
    "#             a.append(i[1])\n",
    "        \n",
    "       \n",
    "#         continue\n",
    "# for i in data2:\n",
    "#     if i[0]==6866:\n",
    "#         a.append(i[1])\n",
    "# negative.append(a)\n",
    "# negative=pd.DataFrame(negative)\n",
    "# negative.to_csv(\"./data/L_DMF_input.test.negative\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 全部做对的题目\n",
    "# data2 = pd.read_csv(r\"C:\\Users\\wangxin\\Desktop\\remmend_DKT_Apriori_DMF\\data\\L_DMF_input.csv\",sep=\",\").to_numpy()\n",
    "# a=[]\n",
    "# for i in data2:\n",
    "    \n",
    "#     if i[2]==1:\n",
    "#         a.append(i)\n",
    "# correct=a\n",
    "# correct=pd.DataFrame(correct)\n",
    "# correct.to_csv('./data/L_DMF_input_correct.rating',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 生成negative，以所有题目作为候选习题\n",
    "# a=[]\n",
    "# b=[]\n",
    "# for j in range(1,91):\n",
    "#         a.append(j)\n",
    "# for i in range(1,6867):\n",
    "    \n",
    "#     b.append(a)\n",
    "# b=pd.DataFrame(b)\n",
    "# print(b)\n",
    "# b.to_csv(\"./data/L_DMF_input.test.negative\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import fabs\n",
    "# import tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import random\n",
    "# data3 = pd.read_csv('./data/assistment2009.csv',sep=',').to_numpy()\n",
    "# a=0\n",
    "# b=0\n",
    "# c=[]\n",
    "# d=[]\n",
    "# stu=0\n",
    "# for i in data3:\n",
    "#     if stu==i[0]:\n",
    "#         a=a+1\n",
    "#         c.append(i)\n",
    "#     else:\n",
    "#         # print('a:',a)\n",
    "#         # print(\"stu:\",stu)\n",
    "#         # print('i[0]:',i[0])\n",
    "#         if a>100:\n",
    "#             b=b+1\n",
    "#             d.append(c)\n",
    "#         a=0\n",
    "#         stu=i[0]\n",
    "# print('b:',b)\n",
    "# d=pd.DataFrame(d)\n",
    "# tqdm.tqdm(d.to_csv('./data/assistment2009_100.csv',index=False))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 李磊测试集的生成\n",
    "# import tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import random\n",
    "# data3 = pd.read_csv(r\"C:\\Users\\wangxin\\Desktop\\remmend_DKT_Apriori_DMF\\data\\assistment2009\\assisment2009_DMF_input.csv\",sep=',').to_numpy()\n",
    "# # print(sorted(data3, key=lambda x: x[0]))\n",
    "# data3=sorted(data3, key=lambda x: x[0])\n",
    "# data3=pd.DataFrame(data3)\n",
    "# data3.to_csv('./data/assistment2009.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # print(data3)\n",
    "# data3 = pd.read_csv('./data/assistment2009_ph.csv',sep=',').to_numpy()\n",
    "# print(data3)\n",
    "# stu=0\n",
    "# a=[]#做过的题目\n",
    "# b=[]\n",
    "# test0=[]\n",
    "# train0=[]\n",
    "# for i in tqdm.tqdm(data3):\n",
    "\n",
    "#     # print(\"i[0]:\",i[0])\n",
    "#     if stu==int(i[0]):\n",
    "#         # test0.append(i)\n",
    "#         a.append(i)             #暂存\n",
    "#         # print(\"a:\",a)\n",
    "#         b=i\n",
    "#         # stu=i[0]\n",
    "#     else:\n",
    "#         # train0.append(i)\n",
    "#         if len(a)>=2:               #学习记录至少两条的情况\n",
    "#                 # 随机取出一条\n",
    "        \n",
    "#         # print(\"len(a):\",len(a))\n",
    "#         # print(\"c\",c)\n",
    "#             c=random.randint(0,len(a)-1)\n",
    "#             test0.append(a[c])              #放到测试集\n",
    "#             for x in range(len(a)):\n",
    "#                 if x!=c:\n",
    "#                     train0.append(a[x])    #遍历剩余的，放到训练集\n",
    "#                 # else:\n",
    "#                 #     continue\n",
    "            \n",
    "#         a=[]\n",
    "#         a.append(i) \n",
    "\n",
    "#         # else:\n",
    "#         #     # print(type(i))\n",
    "#         #     # # print(type(a))\n",
    "#         #     # print(i)\n",
    "#         #     # print(a)\n",
    "#         #     # print(np.array(a))\n",
    "#         #     test0.append(a[0])\n",
    "#         #     train0.append(a[0])\n",
    "#         #     a=[]\n",
    "#         #     a.append(i) \n",
    "    \n",
    "#         stu=i[0]\n",
    "#         # print(\"stu:\",stu)\n",
    "# x=[]\n",
    "# y=[]\n",
    "# for i in data3:\n",
    "#     if i[0]==4216:\n",
    "#         x.append(i)\n",
    "# c=random.randint(0,len(x)-1)        #随机取出一条\n",
    "#         # print(\"len(a):\",len(a))\n",
    "#         # print(\"c\",c)\n",
    "# test0.append(x[c])              #放到测试集\n",
    "# for j in range(len(x)):\n",
    "#     if j!=c:\n",
    "#         train0.append(x[j])\n",
    "# # print(pd.DataFrame(test0))\n",
    "# # print(train0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 别忘了改文件位置\n",
    "# train0=pd.DataFrame(train0)\n",
    "# train0.to_csv('./data/assistment2009.train.rating',index=False)\n",
    "# # train0.to_csv('./data/L_DMF_input.train.csv',index=False)\n",
    "# test0=pd.DataFrame(test0)\n",
    "# test0.to_csv('./data/assistment2009.test.rating',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # assistment2009,negative的生成\n",
    "# import tqdm\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# data2 = pd.read_csv('./data/assistment2009_ph.csv',sep=',').to_numpy()\n",
    "# negative=[]     #没做过的题题目\n",
    "# old_t=[]        #做过的题目\n",
    "# stu=0          #学生编号\n",
    "# all_t=[]        #所有题目\n",
    "# a=0\n",
    "# for i in tqdm.tqdm(data2):\n",
    "    \n",
    "#     all_t.append(i[1])\n",
    "# print(len(set(all_t)))\n",
    "# for i in tqdm.tqdm(data2):\n",
    "#     if stu==i[0]:\n",
    "#         old_t.append(i[1])\n",
    "#     else:\n",
    "#         stu=i[0]\n",
    "#         negative.append(set(all_t)-set(old_t))\n",
    "#         old_t=[]\n",
    "        \n",
    "#         # negative.append(i[1])\n",
    "#         continue\n",
    "\n",
    "# negative.append(set(all_t)-set(old_t))    \n",
    "# print(len(negative))\n",
    "# negative=pd.DataFrame(negative)\n",
    "# negative.to_csv(\"./data/assistment2009_all.test.negative\",index=False)\n",
    "# # print(negative.shape)\n",
    "# # print(negative)\n",
    "# # negative=set(negative)\n",
    "# # print(negative)\n",
    "# # print(old_t)\n",
    "# # print(len(negative))\n",
    "# # print(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.train_ratings, self.train_num_users, self.train_num_items = self.load_train_rating_file_as_list(path + \".train.rating\")\n",
    "        self.test_ratings, self.test_num_users, self.test_num_items = self.load_test_rating_file_as_list(path + \".test.rating\")\n",
    "        self.num_users = max(self.train_num_users, self.test_num_users)\n",
    "        self.num_items = max(self.train_num_items, self.test_num_items)\n",
    "        self.test_negative = self.load_negative_file(path + \".test.negative\")\n",
    "        self.user_item_rating_indices = self.get_user_item_matrix_indices()\n",
    "        self.user_indices, self.item_incides, self.rating_data = self.user_item_rating_indices\n",
    "        # print(\"len(self.test_ratings)\")\n",
    "        # print(len(self.test_ratings))\n",
    "        # print(\"len(self.test_negative)\")\n",
    "        # print(len(self.test_negative))\n",
    "        assert len(self.test_ratings) == len(self.test_negative)\n",
    "        self.train_dict = self.get_train_dict()\n",
    "\n",
    "    # def load_test_rating_file_as_list(self, filename):\n",
    "    #     test_ratings = []\n",
    "    #     num_users, num_items = 0, 0\n",
    "    #     # with open(filename, \"r\") as f:\n",
    "    #     #     line = f.readline()\n",
    "    #     #     while line != None and line != \"\":\n",
    "    #     #         arr = line.split(\" \")\n",
    "    #     #         # \n",
    "    #     #         print(\"arr0:\")\n",
    "    #     #         print(arr)\n",
    "    #     #         # \n",
    "        \n",
    "    #     a=pd.read_csv(filename,sep=\" \").to_numpy()\n",
    "    #     for i in a:\n",
    "    #         user, item = int(i[0]), int(i[1])\n",
    "        \n",
    "    #         num_users = max(num_users, user)\n",
    "    #         num_items = max(num_items, item)\n",
    "    #         test_ratings.append([user, item])\n",
    "            \n",
    "    #     test_num_users = num_users + 1\n",
    "    #     test_num_items = num_items + 1\n",
    "    #     # print(\"test_ratings shape:\")\n",
    "    #     # print(len(test_ratings))\n",
    "    #     print(\"test_ratings\")\n",
    "        \n",
    "       \n",
    "    #     print(test_ratings)\n",
    "    #     print(\"test_num_users\")\n",
    "    #     print(test_num_users)\n",
    "        \n",
    "    #     print(\"test_num_items\") \n",
    "    #     print(test_num_items) \n",
    "    #     return test_ratings, test_num_users, test_num_items\n",
    "    \n",
    "    # def load_negative_file(self, filename):\n",
    "    #     negativeList = []\n",
    "    #     # with open(filename, \"r\") as f:\n",
    "    #     #     line = f.readline()\n",
    "    #     #     while line != None and line != \"\":\n",
    "    #     #         arr = line.split(\" \")\n",
    "    #     #         # \n",
    "    #     #         print(\"arr1:\")\n",
    "    #     #         print(arr)\n",
    "    #     #         # \n",
    "    #     # a=pd.read_csv(filename,sep=\" \").to_numpy()\n",
    "    #     # for i in a:\n",
    "    #     data=[]\n",
    "    #     with open(filename, 'r',encoding='utf-8-sig') as f_input:\n",
    "    #         for line in f_input:\n",
    "    #             data.append(list(line.strip().split(' ')))\n",
    "    #     print(data)\n",
    "    #     negatives = []\n",
    "    #     for x in data[0: ]:\n",
    "                \n",
    "    #         negatives.append(int(x))\n",
    "               \n",
    "    #         negativeList.append(negatives)\n",
    "           \n",
    "    #     # print(\"negativeList len:\")\n",
    "    #     # print(len(negativeList))\n",
    "    #     return negativeList\n",
    "    def load_test_rating_file_as_list(self, filename):\n",
    "        test_ratings = []\n",
    "        num_users, num_items = 0, 0\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split()\n",
    "                user, item = int(arr[0]), int(arr[1])\n",
    "                num_users = max(num_users, user)\n",
    "                num_items = max(num_items, item)\n",
    "                test_ratings.append([user, item])\n",
    "                line = f.readline()\n",
    "            test_num_users = num_users + 1\n",
    "            test_num_items = num_items + 1\n",
    "        # print(\"test_ratings shape:\")\n",
    "        # print(len(test_ratings))\n",
    "            #     # print(\"test_ratings shape:\")\n",
    "        # print(len(test_ratings))\n",
    "    #     print(\"test_ratings\")\n",
    "        \n",
    "       \n",
    "    #     print(test_ratings)\n",
    "    #     print(\"test_num_users\")\n",
    "    #     print(test_num_users)\n",
    "        \n",
    "    #     print(\"test_num_items\") \n",
    "    # #     print(test_num_items) \n",
    "        return test_ratings, test_num_users, test_num_items\n",
    "    def load_negative_file(self, filename):\n",
    "        negativeList = []\n",
    "        with open(filename, \"r\") as f:\n",
    "            line = f.readline()\n",
    "            while line != None and line != \"\":\n",
    "                arr = line.split()\n",
    "                negatives = []\n",
    "                # \n",
    "                # print(arr)\n",
    "                for x in arr[1: ]:\n",
    "                    negatives.append(int(x))\n",
    "                negativeList.append(negatives)\n",
    "                line = f.readline()\n",
    "        # print(\"negativeList len:\")\n",
    "        # print(len(negativeList))\n",
    "        return negativeList\n",
    "\n",
    "    def load_train_rating_file_as_list(self, filename):\n",
    "        '''\n",
    "            return: [[user, item, rating]]\n",
    "        '''\n",
    "        # Get number of users and items\n",
    "        num_users, num_items = 0, 0\n",
    "        # with open(filename, \"r\") as f:\n",
    "        #     line = f.readline()\n",
    "        #     max_items = 0\n",
    "        #     while line != None and line != \"\":\n",
    "        #         arr = line.split(\" \")\n",
    "        #         # \n",
    "        #         print(\"arr2:\")\n",
    "        #         print(arr)\n",
    "        #         # \n",
    "        a=pd.read_csv(filename,sep=\" \").to_numpy()\n",
    "        for i in a:\n",
    "            u, i = int(i[0]), int(i[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "    \n",
    "        train_num_users = num_users + 1\n",
    "        train_num_items = num_items + 1\n",
    "        # Construct matrix\n",
    "        train_ratings = []\n",
    "        a=pd.read_csv(filename,sep=\" \").to_numpy()\n",
    "        for i in a:\n",
    "            user, item, rating = int(i[0]), int(i[1]), i[2]\n",
    "            train_ratings.append([user, item, rating])\n",
    "            \n",
    "            # print(\"train_ratings:\")\n",
    "            # print(train_ratings)\n",
    "        return train_ratings, train_num_users, train_num_items\n",
    "\n",
    "    def get_user_item_matrix_indices(self):\n",
    "        user_indices, item_indices, ratings = [], [], []\n",
    "        for i in self.train_ratings:\n",
    "            user_indices.append(i[0])\n",
    "            item_indices.append(i[1])\n",
    "            ratings.append(1)\n",
    "\n",
    "        return [np.array(user_indices), np.array(item_indices), np.array(ratings)]\n",
    "\n",
    "    def get_user_item_interact_list(self):\n",
    "        user_item_interact = []\n",
    "        user, item, rate = [], [], []\n",
    "        user_idx = int(0)\n",
    "        for i in self.train_ratings:\n",
    "            # print(i[0])\n",
    "            if user_idx != i[0]:\n",
    "                user_item_interact.append([user, item, rate])\n",
    "                user_idx += 1\n",
    "                user, item, rate = [], [], []\n",
    "            else:\n",
    "                user.append(i[0])\n",
    "                item.append(i[1])\n",
    "                rate.append(i[2])\n",
    "            # print(\"user_item_interact:\")\n",
    "            # print(user_item_interact)\n",
    "        return user_item_interact\n",
    "\n",
    "    def get_item_user_interact_list(self):\n",
    "        item_user_interact = []\n",
    "        user, item, rate = [], [], []\n",
    "        item_idx = 0\n",
    "        for i in self.train_ratings:\n",
    "            if item_idx != i[1]:\n",
    "                item_user_interact.append([user, item, rate])\n",
    "                item_idx += 1\n",
    "                user, item, rate = [], [], []\n",
    "            else:\n",
    "                user.append(i[0])\n",
    "                item.append(i[1])\n",
    "                rate.append(i[2])\n",
    "        # print(\"item_user_interact:\")\n",
    "        # print(item_user_interact)\n",
    "        return item_user_interact\n",
    "\n",
    "    def get_train_instances(self, num_negative):\n",
    "        user, item, rate = [], [], []\n",
    "        for i in self.train_ratings:\n",
    "            user.append(i[0])\n",
    "            item.append(i[1])\n",
    "            rate.append(1)\n",
    "            # print(\"添加负例前[np.array(user), np.array(item), np.array(rate)]:\")\n",
    "            # print([np.array(user), np.array(item), np.array(rate)])\n",
    "            for t in range(num_negative):\n",
    "                j = np.random.randint(self.num_items)\n",
    "                while (i[0], j) in self.train_dict:\n",
    "                    j = np.random.randint(self.num_items)\n",
    "                user.append(i[0])\n",
    "                item.append(j)\n",
    "                rate.append(0)\n",
    "            # print(\"添加负例后[np.array(user), np.array(item), np.array(rate)]:\")\n",
    "            # print([np.array(user), np.array(item), np.array(rate)])\n",
    "        return [np.array(user), np.array(item), np.array(rate)]\n",
    "\n",
    "\n",
    "    def get_user_and_item_matrix(self):\n",
    "        rom = np.random.rand(1, 100)\n",
    "        user_matrix = self.user_item_matrix\n",
    "        item_matrix = self.user_item_matrix.T\n",
    "        return user_matrix, item_matrix\n",
    "\n",
    "    def get_train_dict(self):\n",
    "        data_dict = {}\n",
    "        for i in self.train_ratings:\n",
    "            data_dict[(i[0], i[1])] = i[2]\n",
    "        return data_dict\n",
    "\n",
    "    def get_item_sparse_matrix(self):\n",
    "        num_users, num_items = self.num_users, self.num_items\n",
    "        user_indices, item_incides, rating_data = self.user_item_rating_indices\n",
    "        item_sparse_matrix = csr_matrix((rating_data, (item_incides, user_indices)), shape=(num_items, num_users))\n",
    "        return item_sparse_matrix\n",
    "\n",
    "    def get_user_sparse_matrix(self):\n",
    "        user_sparse_matrix = self.get_item_sparse_matrix().T\n",
    "        return user_sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header:data, load time:8.8, user:4217,train:675726 item:26688, test:4097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading data\n",
    "t1 = time()\n",
    "dataset = Dataset(path + dataset_name)\n",
    "train, test_rating, test_negative = dataset.get_train_instances(\n",
    "    num_negative), dataset.test_ratings, dataset.test_negative\n",
    "num_users, num_items = dataset.num_users, dataset.num_items\n",
    "t2 = time()\n",
    "print(\"header:data, load time:{:.1f}, user:{:d},train:{:d} item:{:d}, test:{:d}\"\n",
    "        .format(t2 - t1, num_users, \n",
    "        len(train[0]), num_items, len(test_rating)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义loss函数\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mse_loss = torch.mean(torch.pow((x - y), 2)) # x与y相减后平方，求均值即为MSE\n",
    "        return mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMF(\n",
      "  (linear_user_1): Linear(in_features=26688, out_features=64, bias=True)\n",
      "  (linear_item_1): Linear(in_features=4217, out_features=64, bias=True)\n",
      "  (user_fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (item_fc_layers): ModuleList(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = utils.UserItemRatingDataset(train[0], train[1], train[2])\n",
    "train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Build model\n",
    "model = DMF(num_users, num_items, layers, dataset)\n",
    "model = model.to(device)\n",
    "# 更改loss函数\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header:class:DMF,dataset:assistment2009, batch_size:256, epochs:20, latent_dim:64, num_negative:1, topK:10, lr:0.0001, reg:0.0,loss:MSELoss\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_name = str(model.__class__)[17:][:-2]\n",
    "loss_name = str(criterion.__class__)[17 + 13:][:-2]\n",
    "print(\n",
    "    'header:class:{},dataset:{}, batch_size:{}, epochs:{}, latent_dim:{}, num_negative:{}, topK:{}, lr:{}, reg:{},loss:{}'\n",
    "    .format(model_name, dataset_name, batch_size, epochs, latent_dim, num_negative, topK, learning_rate, reg,\n",
    "            loss_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.py\n",
    "\n",
    "import math\n",
    "import heapq # for retrieval topK\n",
    "import multiprocessing\n",
    "from operator import index\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 是否激活cuda\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global variables that are shared across processes\n",
    "_model = None\n",
    "_testRatings = None\n",
    "_testNegatives = None\n",
    "_K = None\n",
    "\n",
    "# '''\n",
    "# Evaluate the performance of Top-K recommendation:\n",
    "#     Protocol: leave-1-out evaluation\n",
    "#     Measures: Hit Ratio and NDCG\n",
    "#     (more details are in: Xiangnan He, et al. Fast Matrix Factorization for Online Recommendation with Implicit Feedback. SIGIR'16)\n",
    "# @author: hexiangnan\n",
    "# '''\n",
    "def evaluate_model(model, testRatings, testNegatives, K, num_thread,epoch):\n",
    "    \"\"\"\n",
    "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
    "    Return: score of each test rating.\n",
    "    \"\"\"\n",
    "    global _model\n",
    "    global _testRatings\n",
    "    global _testNegatives\n",
    "    global _K\n",
    "    global _dataset\n",
    "    global _epoch\n",
    "    _model = model\n",
    "    _testRatings = testRatings\n",
    "    _testNegatives = testNegatives\n",
    "    _K = K\n",
    "    _epoch = epoch\n",
    "\n",
    "    hits, ndcgs, mrrs = [],[], []\n",
    "    if(num_thread > 1): # Multi-thread\n",
    "        pool = multiprocessing.Pool(processes=num_thread)\n",
    "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        hits = [r[0] for r in res]\n",
    "        ndcgs = [r[1] for r in res]\n",
    "        return (np.mean(hits), np.mean(ndcgs))\n",
    "    # Single thread\n",
    "    for idx in range(len(_testRatings)):\n",
    "        (hr,ndcg, mrr) = eval_one_rating(idx,epoch)\n",
    "        hits.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "        mrrs.append(mrr)\n",
    "    return (np.mean(hits), np.mean(ndcgs))\n",
    "\n",
    "def eval_one_rating(idx,epoch):\n",
    "    rating = _testRatings[idx]\n",
    "\n",
    "    # print(\"rating:\")\n",
    "    # print(rating)\n",
    "    # ####改动\n",
    "    # items = _testNegatives[idx][0:999]\n",
    "    items = _testNegatives[idx]\n",
    "\n",
    "    # print(\"items:\")\n",
    "    # print(items)\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    # print(\"u  rating[0]:\")\n",
    "    # print(u)\n",
    "    # print(\"rating[1]:\")\n",
    "    # print(gtItem)\n",
    "    items.append(gtItem)\n",
    "    # print(\"赋值后的items:\")\n",
    "    # print(items)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype='int64')\n",
    "    # print(\"users:\")\n",
    "    # print(users)\n",
    "    # 可将编码器置于此处，代替Longtensor，\n",
    "\n",
    "    batch_users, batch_items = torch.LongTensor(users), torch.LongTensor(items)\n",
    "    # print(\"batch_users:\")\n",
    "    # print(batch_users)\n",
    "    # print(\"batch_items:\")\n",
    "    # print(batch_items)\n",
    "\n",
    "\n",
    "    tensor_users, tensor_items = batch_users.to(device), batch_items.to(device)\n",
    "    y_pred = _model(tensor_users, tensor_items) # model predict\n",
    "    # print(y_pred)\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    # \n",
    "    # print (\"y_pred.shape:\")\n",
    "    # print(y_pred.shape)\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = y_pred[i]\n",
    "        \n",
    "    # 推荐指数的获取\n",
    "    score_index=[]\n",
    "    map_item_score_sorted = sorted(map_item_score.items(), key=lambda x: x[1],reverse=True)\n",
    "    # print(map_item_score_sorted)\n",
    "    exerciseID=list(dict(map_item_score_sorted).keys())\n",
    "    recommendation_rate = list(dict(map_item_score_sorted).values())\n",
    "    # print(exerciseID)\n",
    "    # print(recommendation_rate)\n",
    "    # d=[]\n",
    "    # for i in range(len(exerciseID)):\n",
    "    #     d.append([exerciseID[i],recommendation_rate[i]])\n",
    "    # print(d)\n",
    "    # print(map_item_score_sorted)\n",
    "    # print(exerciseID)\n",
    "    # for i in exerciseID:\n",
    "    \n",
    "    if epoch== 5:\n",
    "        exercise=[]\n",
    "        exercise0=[]\n",
    "        index=[]\n",
    "        index0=[]\n",
    "        for i in exerciseID:\n",
    "            exercise.append(i)\n",
    "        exercise0.append(exercise)\n",
    "        for j in recommendation_rate:\n",
    "            index.append(j)\n",
    "        index0.append(index)\n",
    "    # print(exercise)\n",
    "    # print(index)\n",
    "        \n",
    "        exerciseID=pd.DataFrame(exercise0)\n",
    "        recommendation_rate=pd.DataFrame(index0)\n",
    "        exerciseID.to_csv('C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/result/assistment2009/DMF_03_05_exercise.csv',mode='a',index=False)\n",
    "        recommendation_rate.to_csv('C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/result/assistment2009/DMF_03_05_index.csv',mode='a',index=False)\n",
    "        # with open(r\"C:\\Users\\wangxin\\Desktop\\remmend_DKT_Apriori_DMF\\result\\assistment2009\\DMF_100_exercise.csv\",mode='a',) as f:\n",
    "        #     # print(\"学生id：\",rating[0],file=f)\n",
    "        #     print(exerciseID,file=f)\n",
    "    # for i in recommendation_rate:\n",
    "    #     print(i)\n",
    "    # print(recommendation_rate)\n",
    "    # with open(\"C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/result\",mode='a',) as f:\n",
    "    #     # print(\"map_item_score:\")\n",
    "    #     print(\"学生id:\",rating[0],file=f)\n",
    "    #     # exerciseID=list(map_item_score.keys).sort()\n",
    "        \n",
    "    #     \n",
    "        # print()\n",
    "    items.pop()\n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
    "    hr = getHitRatio(ranklist, gtItem)\n",
    "    ndcg = getNDCG(ranklist, gtItem)\n",
    "    mrr = getMRR(ranklist, gtItem)\n",
    "    # print(hr, ndcg)\n",
    "    return (hr, ndcg, mrr)\n",
    "def testmodel(idx):\n",
    "    rating = _testRatings[idx]\n",
    "\n",
    "    # print(\"rating:\")\n",
    "    # print(rating)\n",
    "    # items = _testNegatives[idx][0:999]\n",
    "    items = _testNegatives[idx]\n",
    "\n",
    "    # print(\"items:\")\n",
    "    # print(items)\n",
    "    u = rating[0]\n",
    "    gtItem = rating[1]\n",
    "    # print(\"u  rating[0]:\")\n",
    "    # print(u)\n",
    "    # print(\"rating[1]:\")\n",
    "    # print(gtItem)\n",
    "    items.append(gtItem)\n",
    "    # print(\"赋值后的items:\")\n",
    "    # print(items)\n",
    "    # Get prediction scores\n",
    "    map_item_score = {}\n",
    "    users = np.full(len(items), u, dtype='int64')\n",
    "    # print(\"users:\")\n",
    "    # print(users)\n",
    "    # 可将编码器置于此处，代替Longtensor，\n",
    "\n",
    "    batch_users, batch_items = torch.LongTensor(users), torch.LongTensor(items)\n",
    "    # print(\"batch_users:\")\n",
    "    # print(batch_users)\n",
    "    # print(\"batch_items:\")\n",
    "    # print(batch_items)\n",
    "\n",
    "\n",
    "    tensor_users, tensor_items = batch_users.to(device), batch_items.to(device)\n",
    "    y_pred = _model(tensor_users, tensor_items) # model predict\n",
    "    y_pred = y_pred.cpu()\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "def getHitRatio(ranklist, gtItem):\n",
    "    for item in ranklist:\n",
    "        if item == gtItem:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return math.log(2) / math.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def getMRR(ranklist, gtItem):\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return float(1.0) / (i+1)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Init performance\n",
    "epoch=0\n",
    "t1 = time()\n",
    "(hr, ndcg) = evaluate_model(model, test_rating, test_negative, topK, evaluation_threads,epoch)\n",
    "t2 = time()\n",
    "    \n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,train_time:20.9s, HR:0.0366, NDCG:0.0255, test_time:0.0s, loss:0.125765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [02:22<45:08, 142.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1,train_time:20.9s, HR:0.2853, NDCG:0.2003, test_time:121.6s, loss:0.125765\n",
      "epoch:2,train_time:20.7s, HR:0.2853, NDCG:0.2003, test_time:0.0s, loss:0.093239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [04:43<42:24, 141.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2,train_time:20.7s, HR:0.3510, NDCG:0.2536, test_time:119.8s, loss:0.093239\n",
      "epoch:3,train_time:20.6s, HR:0.3510, NDCG:0.2536, test_time:0.0s, loss:0.098189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [07:02<39:48, 140.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3,train_time:20.6s, HR:0.3686, NDCG:0.2710, test_time:118.9s, loss:0.098189\n",
      "epoch:4,train_time:20.4s, HR:0.3686, NDCG:0.2710, test_time:0.0s, loss:0.077938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [09:22<37:22, 140.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4,train_time:20.4s, HR:0.3727, NDCG:0.2784, test_time:119.1s, loss:0.077938\n",
      "epoch:5,train_time:20.6s, HR:0.3727, NDCG:0.2784, test_time:0.0s, loss:0.104349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [31:10<2:20:23, 561.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5,train_time:20.6s, HR:0.3842, NDCG:0.2852, test_time:1288.2s, loss:0.104349\n",
      "epoch:6,train_time:22.5s, HR:0.3842, NDCG:0.2852, test_time:0.0s, loss:0.090299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [33:28<1:37:25, 417.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:6,train_time:22.5s, HR:0.3803, NDCG:0.2859, test_time:115.5s, loss:0.090299\n",
      "epoch:7,train_time:21.6s, HR:0.3803, NDCG:0.2859, test_time:0.0s, loss:0.093159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [35:44<1:10:28, 325.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7,train_time:21.6s, HR:0.3876, NDCG:0.2884, test_time:113.6s, loss:0.093159\n",
      "epoch:8,train_time:22.2s, HR:0.3876, NDCG:0.2884, test_time:0.0s, loss:0.090519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [38:04<53:15, 266.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:8,train_time:22.2s, HR:0.3979, NDCG:0.2978, test_time:117.7s, loss:0.090519\n",
      "epoch:9,train_time:19.9s, HR:0.3979, NDCG:0.2978, test_time:0.0s, loss:0.075464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [40:23<41:30, 226.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:9,train_time:19.9s, HR:0.3981, NDCG:0.2959, test_time:119.0s, loss:0.075464\n",
      "epoch:10,train_time:20.7s, HR:0.3981, NDCG:0.2959, test_time:0.0s, loss:0.086165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [42:44<33:20, 200.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10,train_time:20.7s, HR:0.4018, NDCG:0.2998, test_time:120.4s, loss:0.086165\n",
      "epoch:11,train_time:19.9s, HR:0.4018, NDCG:0.2998, test_time:0.0s, loss:0.086852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [45:01<27:09, 181.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11,train_time:19.9s, HR:0.4020, NDCG:0.2979, test_time:117.9s, loss:0.086852\n",
      "epoch:12,train_time:22.2s, HR:0.4020, NDCG:0.2979, test_time:0.0s, loss:0.088448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [47:20<22:23, 167.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:12,train_time:22.2s, HR:0.3981, NDCG:0.3019, test_time:115.9s, loss:0.088448\n",
      "epoch:13,train_time:20.2s, HR:0.3981, NDCG:0.3019, test_time:0.0s, loss:0.098209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [49:41<18:39, 159.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:13,train_time:20.2s, HR:0.3964, NDCG:0.2994, test_time:120.9s, loss:0.098209\n",
      "epoch:14,train_time:21.0s, HR:0.3964, NDCG:0.2994, test_time:0.0s, loss:0.090917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [52:00<15:22, 153.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:14,train_time:21.0s, HR:0.4008, NDCG:0.2966, test_time:118.6s, loss:0.090917\n",
      "epoch:15,train_time:20.0s, HR:0.4008, NDCG:0.2966, test_time:0.0s, loss:0.085741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [54:18<12:24, 148.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:15,train_time:20.0s, HR:0.3993, NDCG:0.3022, test_time:117.3s, loss:0.085741\n",
      "epoch:16,train_time:21.7s, HR:0.3993, NDCG:0.3022, test_time:0.0s, loss:0.113931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [56:35<09:40, 145.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:16,train_time:21.7s, HR:0.3964, NDCG:0.2979, test_time:115.0s, loss:0.113931\n",
      "epoch:17,train_time:21.0s, HR:0.3964, NDCG:0.2979, test_time:0.0s, loss:0.077933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [58:50<07:07, 142.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:17,train_time:21.0s, HR:0.3974, NDCG:0.2988, test_time:114.9s, loss:0.077933\n",
      "epoch:18,train_time:21.1s, HR:0.3974, NDCG:0.2988, test_time:0.0s, loss:0.077106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [1:01:07<04:41, 140.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:18,train_time:21.1s, HR:0.3961, NDCG:0.2985, test_time:115.7s, loss:0.077106\n",
      "epoch:19,train_time:21.0s, HR:0.3961, NDCG:0.2985, test_time:0.0s, loss:0.081155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [1:03:23<02:19, 139.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:19,train_time:21.0s, HR:0.3981, NDCG:0.2985, test_time:114.7s, loss:0.081155\n",
      "epoch:20,train_time:20.2s, HR:0.3981, NDCG:0.2985, test_time:0.0s, loss:0.104720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [1:05:38<00:00, 196.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:20,train_time:20.2s, HR:0.4020, NDCG:0.3019, test_time:114.6s, loss:0.104720\n",
      "best epoch:11,HR:0.4020, NDCG:0.2979\n",
      "The best model is saved to C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/pretrain//11.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "                                                                    \n",
    "import code\n",
    "from encodings import utf_8\n",
    "\n",
    "\n",
    "epoch=0\n",
    "model_out_file = 'C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/pretrain/' + '/{}.model'.format(epoch)\n",
    "if args.out > 0:\n",
    "    torch.save(model.state_dict(), model_out_file)\n",
    "\n",
    "    # Train model\n",
    "best_hr, best_ndcg, best_iter, best_epoch = 0, 0, -1, -1\n",
    "count = 0\n",
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    epoch = epoch + 1\n",
    "    t1 = time()\n",
    "    # Generate training instances\n",
    "    train = dataset.get_train_instances(num_negative)\n",
    "    train = utils.UserItemRatingDataset(train[0], train[1], train[2])\n",
    "    train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    # Training\n",
    "    for batch_idx, (user, item, y) in enumerate(train):\n",
    "            # 改动\n",
    "            # user, item, y = user.cuda(), item.cuda(), y.cuda()\n",
    "        user, item, y = user.to(device), item.to(device), y.to(device)\n",
    "            ## forward and backprop\n",
    "        y_hat = model(user, item)\n",
    "        # print(\"y_hat:\")\n",
    "        # print(y_hat.shape)\n",
    "        # print(\"y:\")\n",
    "        # print(y)\n",
    "        # print(\"y.view(-1,1):\")\n",
    "        # print(y.view(-1,1))\n",
    "        loss = criterion(y_hat, y.view(-1, 1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t2 = time()\n",
    "        model.eval()\n",
    "    print('epoch:{},train_time:{:.1f}s, HR:{:.4f}, NDCG:{:.4f}, test_time:{:.1f}s, loss:{:.6f}'\n",
    "            .format(epoch, t2 - t1, hr, ndcg, time() - t2, loss))\n",
    "    # with open('C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/result/L_DMF_result.txt',mode='a',encoding='utf-8') as f:\n",
    "    #     print('epoch:{},train_time:{:.1f}s, HR:{:.4f}, NDCG:{:.4f}, test_time:{:.1f}s, loss:{:.6f}'\n",
    "    #             .format(epoch, t2 - t1, hr, ndcg, time() - t2, loss),file=f)\n",
    "    # Evaluation\n",
    "    if epoch % verbose == 0:\n",
    "        (hr, ndcg) = evaluate_model(model, test_rating, test_negative, topK, evaluation_threads,epoch)\n",
    "        print('epoch:{},train_time:{:.1f}s, HR:{:.4f}, NDCG:{:.4f}, test_time:{:.1f}s, loss:{:.6f}'\n",
    "                .format(epoch, t2 - t1, hr, ndcg, time() - t2, loss))\n",
    "        with open('C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/result/L_DMF_result.txt',mode='a',encoding='utf-8') as f:\n",
    "            print('epoch:{},train_time:{:.1f}s, HR:{:.4f}, NDCG:{:.4f}, test_time:{:.1f}s, loss:{:.6f}'\n",
    "                    .format(epoch, t2 - t1, hr, ndcg, time() - t2, loss),file=f)\n",
    "        if hr > best_hr:\n",
    "            count = 0\n",
    "            best_train_time, best_hr, best_ndcg, best_epoch, best_test_time = t2 - t1, hr, ndcg, epoch, time() - t2\n",
    "                # model_out_file = 'pretrain/' + '/{}-{}-{}-{}-{}-lr_{}-HR_{:.4f}-NDCG_{:.4f}-epoch_{}.model'.format(\n",
    "                #     model_name,\n",
    "                #     dataset_name,\n",
    "                #     latent_dim,\n",
    "                #     layers,\n",
    "                #     num_negative,\n",
    "                #     learning_rate,\n",
    "                #     hr,\n",
    "                #     ndcg,\n",
    "                #     epoch)\n",
    "            model_out_file = 'C:/Users/wangxin/Desktop/remmend_DKT_Apriori_DMF/pretrain/' + '/{}.model'.format(epoch)\n",
    "            if args.out > 0:\n",
    "                torch.save(model.state_dict(), model_out_file)\n",
    "        else:\n",
    "            count += 1\n",
    "        if count == 50:\n",
    "            sys.exit(0)\n",
    "\n",
    "print('best epoch:{},HR:{:.4f}, NDCG:{:.4f}'.format(best_epoch, best_hr, best_ndcg))\n",
    "if args.out > 0:\n",
    "    print(\"The best model is saved to {}\".format(model_out_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py3.7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eda8d4e8ec5d995ac7f6ec7bc64f13839b1e68f65e54dfe9ccfe00cc3d457174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
